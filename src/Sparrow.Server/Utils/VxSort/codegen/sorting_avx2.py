import os
from datetime import datetime
import random

from utils import native_size_map, next_power_of_2
from sorting_isa import SortingISA


class AVX2SortingISA(SortingISA):
    def __init__(self, type, configuration):
        self.vector_size_in_bytes = 32

        self.type = type
        self.compress_writes = False
        self.can_pack = configuration.can_pack
        self.shift = 0

        self.bitonic_size_map = {}
        self._generated_load_partition = []

        for t, s in native_size_map.items():
            self.bitonic_size_map[t] = int(self.vector_size_in_bytes / s)

        self.unsigned_type_map = {
            "int": "uint",
            "uint": "uint",
            "float": "uint",
            "long": "ulong",
            "ulong": "ulong",
            "double": "ulong",
        }

        self.sorting_type_map = {
            "int": "Int32",
            "uint": "UInt32",
            "float": "Int32",
            "long": "Int64",
            "ulong": "UInt64",
            "double": "Int64",
        }

        self.capital_type_map = {
            "int": "Int32",
            "uint": "UInt32",
            "float": "Float",
            "long": "Int64",
            "ulong": "UInt64",
            "double": "Double",
        }

        self.pack_type_map = {
            "long": "int",
            "ulong": "uint",
        }

        self.unroll = configuration.unroll
        self.unroll2 = configuration.unroll // 2
        if self.unroll2 == 1 and self.unroll > 2:
            self.unroll2 = 2
        if self.unroll2 == 1:
            self.unroll2 = None

        self.pack_unroll = configuration.pack_unroll

        self.max_bitonic_sort_vectors = configuration.max_bitonic_sort_vectors
        self.do_prefetch = configuration.do_prefetch
        self.is_debug = configuration.is_debug
        self.namespace = configuration.namespace

    @property
    def max_inner_unroll(self):
        return int((self.max_bitonic_sort_vectors - 3) / 2)

    @property
    def safe_inner_unroll(self):
        return self.unroll if self.max_inner_unroll > self.unroll else self.max_inner_unroll

    def vector_size(self):
        return self.bitonic_size_map[self.type]

    def vector_type(self):
        return "V"

    @classmethod
    def supported_types(cls):
        return native_size_map.keys()

    def i2d(self, v):
        t = self.type
        if t == "double":
            return v
        elif t == "float":
            return f"s2d({v})"
        return f"i2d({v})"

    def i2s(self, v):
        t = self.type
        if t == "double":
            raise Exception("Incorrect Type")
        elif t == "float":
            return f"i2s({v})"
        return v

    def d2i(self, v):
        t = self.type
        if t == "double":
            return v
        elif t == "float":
            return f"d2s({v})"
        return f"d2i<{t}>({v})"

    def s2i(self, v):
        t = self.type
        if t == "double":
            raise Exception("Incorrect Type")
        elif t == "float":
            return f"s2i<int>({v})"
        return v

    def generate_prologue(self, f):
        g = self
        s = f"""{self.autogenerated_blabber()}
using System;
using System.Diagnostics;
using System.Runtime.CompilerServices;
using System.Runtime.InteropServices;
using System.Runtime.Intrinsics;
using System.Runtime.Intrinsics.X86;
using static System.Runtime.Intrinsics.X86.Avx;
using static System.Runtime.Intrinsics.X86.Avx2;
using static System.Runtime.Intrinsics.X86.Sse2;
using static System.Runtime.Intrinsics.X86.Sse41;
using static System.Runtime.Intrinsics.X86.Sse42;

namespace {g.namespace}
{{"""
        print(s, file=f)

    def generate_epilogue(self, f):
        s = f"""}}"""
        print(s, file=f)

    def generate_entry_points(self, f):
        t = self.type
        s = f"""
    using V = Vector256<{t}>;
    internal unsafe partial struct Avx2VectorizedSort
    {{"""
        print(s, file=f)

        self.generate_configuration_struct(f)
        self.generate_scalar_alignments(f)
        self.generate_vectorized_alignments(f)

        self.generate_partition_block(f)

        self.generate_load_and_partition_vectors(f, 1)
        self.generate_load_and_partition_vectors(f, self.unroll2)
        self.generate_load_and_partition_vectors(f, self.max_inner_unroll)
        self.generate_vectorized_partition(f, self.max_inner_unroll)

        if self.safe_inner_unroll != self.max_inner_unroll:
            self.generate_load_and_partition_vectors(f, self.safe_inner_unroll)
            self.generate_vectorized_partition(f, self.safe_inner_unroll)

        self.generate_sort_primitives(f)

        self.generate_pack(f)
        self.generate_unpack(f)
        self.generate_sort(f)
        print(f"""    }}""", file=f)

    def get_small_sort_threshold_elements(self, n='V.Count'):
        return f"""{self.max_bitonic_sort_vectors} * {n}"""

    def get_partition_tmp_size_in_elements(self, n='V.Count'):
        return f"""2 * ({self.unroll} * {n}) + {n} + 4 * {n}"""

    def get_cmpgt_mask(self, vector, pivot):
        t = self.type
        if t == 'int' or t == 'uint' or t == 'float':
            return f"""(uint)MoveMask(CompareGreaterThan({vector}, {pivot}).AsSingle())"""

        return f"""(uint)MoveMask(CompareGreaterThan({vector}, {pivot}).AsDouble())"""


    def get_popcnt(self, value, type='int64'):
        if type == 'int64':
            return f"""Popcnt.X64.PopCount({value})"""
        else:
            return f"""Popcnt.PopCount({value})"""

    def get_store_vec(self, ptr, vector):
        return f"""Store({ptr}, {vector})"""

    def get_broadcast(self, param):
        return f"""Vector256.Create({param})"""

    def get_partition_vector(self, vector, basePtr, mask):
        t = self.type

        if t == 'int':
            return f"""PermuteVar8x32({vector}, ConvertToVector256Int32(LoadVector128({basePtr} + {mask} * 8)))"""
        if t == 'uint':
            return f"""PermuteVar8x32({vector}.AsInt32(), ConvertToVector256Int32(LoadVector128({basePtr} + {mask} * 8))).AsUInt32()"""
        if t == 'long':
            return f"""PermuteVar8x32({vector}.AsSingle(), ConvertToVector256Int32(LoadVector128({basePtr} + {mask} * 8))).AsInt64()"""
        if t == 'ulong':
            return f"""PermuteVar8x32({vector}.AsSingle(), ConvertToVector256Int32(LoadVector128({basePtr} + {mask} * 8))).AsUInt64()"""
        raise Exception('NotImplemented')

    def get_load_vector(self, ptr, aligned=True):
        return f"""LoadAlignedVector256({ptr})""" if aligned else f"""LoadVector256({ptr})"""

    def get_shift_n_sub(self, shift, v, sub):
        prefix = F"({v} >> {shift})" if shift > 0 else v
        return F"""{prefix} - {sub}"""

    def get_unshift_n_add(self, shift, _from, add):
        s = F'({add} + {_from})'
        if shift > 0:
            s = F'({s} << {shift})'
        return s

    def get_permutation_table_ptr(self):
        table_name = 'perm_table_64'
        if self.sorting_type_map[self.type] == 'Int32':
            table_name = 'perm_table_32'

        return F"(byte*)Unsafe.AsPointer(ref MemoryMarshal.GetReference({table_name}))"

    def generate_partition_block(self, f):

        def generate_comparison_operations(t):
            if t == 'int' or t == 'long' or t == 'float' or t == 'double':
                s = f"""var mask = (ulong){self.get_cmpgt_mask('dataVec', 'p')};"""
            if t == 'uint' or t == 'ulong':
                s = f"""var additionConstant = Vector256.Create(unchecked(({t})-1));
                var mask = (ulong){self.get_cmpgt_mask('Add(dataVec, additionConstant).AsInt64()', 'Add(p, additionConstant).AsInt64()')};"""
            return s

        t = self.type
        method_name = f"""
        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static void partition_block(V dataVec, V p, byte* pBase, ref {t}* left, ref {t}* right)"""

        try:

            s = f"""
            {method_name}
            {{
                {generate_comparison_operations(t)}
    
                // Looks kinda silly, the (ulong) (uint) thingy right?
                // Well, it's making a yucky lemonade out of lemons is what it is.
                // This is a crappy way of making the jit generate slightly less worse code
                // due to: https://github.com/dotnet/runtime/issues/431#issuecomment-568280829
                // To summarize: VMOVMASK is mis-understood as a 32-bit write by the CoreCLR 3.x JIT.
                // It's really a 64 bit write in 64 bit mode, in other words, it clears the entire register.
                // Again, the JIT *should* be aware that the destination register just had it's top 32 bits cleared.
                // It doesn't.
                // This causes a variety of issues, here it's that GetBytePermutation* method is generated
                // with suboptimal x86 code (see above issue/comment).
                // By forcefully clearing the 32-top bits by casting to ulong, we "help" the JIT further down the road
                // and the rest of the code is generated more cleanly.
                // In other words, until the issue is resolved we "pay" with a 2-byte instruction for this useless cast
                // But this helps the JIT generate slightly better code below (saving 3 bytes).
                var maskedDataVec = {self.get_partition_vector('dataVec', 'pBase', 'mask')};
               
                // By "delaying" the PopCount to this stage, it is highly likely (I don't know why, I just know it is...)
                // that the JIT will emit a POPCNT X,X instruction, where X is now both the source and the destination
                // for PopCount. This means that there is no need for clearing the destination register (it would even be
                // an error to do so). This saves about two bytes in the instruction stream.
                var pc = -(long)(int){self.get_popcnt('mask', 'int64')};
    
                {self.get_store_vec('left', 'maskedDataVec')};
                {self.get_store_vec('right', 'maskedDataVec')};
                                             
                // I comfortably ignored having negated the PopCount result after casting to (long)
                // The reasoning behind this is that be storing the PopCount as a negative
                // while also expressing the pointer bumping (next two lines) in this very specific form that
                // it is expressed: a summation of two variables with an optional constant (that CAN be negative)
                // We are allowing the JIT to encode this as two LEA opcodes in x64: https://www.felixcloutier.com/x86/lea
                // This saves a considerable amount of space in the instruction stream, which are then exploded
                // when this block is unrolled. All in all this is has a very clear benefit in perf while decreasing code
                // size.
                // TODO: Currently the entire sorting operation generates a right-hand popcount that needs to be negated
                //       If/When I re-write it to do left-hand comparison/pop-counting we can save another two bytes
                //       for the negation operation, which will also do its share to speed things up while lowering
                //       the native code size, yay for future me!
                right = right + pc;
                left = left + pc + V.Count; // default(MP).N;
            }}
    """
        except:
            s = f"""{method_name} {{ throw new NotImplementedException(); }}"""

        print(s, file=f)

    def generate_vectorized_alignments(self, f):
        g = self
        t = self.type
        compress_writes = self.compress_writes and (t == 'long' or t == 'ulong')

        def generate_comparison_operations(t):
            if t == 'int' or t == 'long' or t == 'float' or t == 'double':
                s = f"""var rtMask = {self.get_cmpgt_mask('RT0', 'p')}; //default(MT).get_cmpgt_mask(RT0, p);
                var ltMask = {self.get_cmpgt_mask('LT0', 'p')}; //default(MT).get_cmpgt_mask(LT0, p);"""
            if t == 'uint' or t == 'ulong':
                s = f"""var additionConstant = Vector256.Create(unchecked(({t})-1));
                var signedP = Add(p, additionConstant).AsInt64();
                var rtMask = {self.get_cmpgt_mask('Add(RT0, additionConstant).AsInt64()', 'signedP')}; //default(MT).get_cmpgt_mask(RT0, p);
                var ltMask = {self.get_cmpgt_mask('Add(LT0, additionConstant).AsInt64()', 'signedP')}; //default(MT).get_cmpgt_mask(LT0, p);"""

            return s

        def generate_permutations_without_compress():

            return f"""
            RT0 = {self.get_partition_vector('RT0', 'pBase', 'rtMask')};  
            {self.get_store_vec('tmpRight', 'RT0')};                     

            LT0 = {self.get_partition_vector('LT0', 'pBase', 'ltMask')};            
            {self.get_store_vec('tmpLeft', 'LT0')};

            tmpRight -= rtPopCountRightPart & rai;
            readRight += (rightAlign - N) & rai;

            {self.get_store_vec('tmpRight', 'LT0')};
            tmpRight -= ltPopCountRightPart & lai;
            tmpLeft += ltPopCountLeftPart & lai;
            tmpStartLeft += -leftAlign & lai;
            readLeft += (leftAlign + N) & lai;

            {self.get_store_vec('tmpLeft', 'RT0')};
            tmpLeft += rtPopCountLeftPart & rai;
            tmpStartRight -= rightAlign & rai;"""

        def generate_permutations_with_compress():
            raise Exception('NotImplemented')

        method_name = f"""
        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static void align_vectorized(
            {t}* left, {t}* right,
            int leftAlign, int rightAlign,
            in V p,
            byte* pBase,
            ref {t}* readLeft, ref {t}* readRight,
            ref {t}* tmpStartLeft, ref {t}* tmpLeft,
            ref {t}* tmpStartRight, ref {t}* tmpRight)"""
        try:
            s = f"""        
            {method_name}
            {{
                // PERF: CompressWrite support is been treated as a constant because we make sure the caller
                //       treats that parameter already as a constant @ JIT time causing a cascade.
        
                int N = V.Count;
        
                var rai = ~((rightAlign - 1) >> 31);
                var lai = leftAlign >> 31;
                var preAlignedLeft = left + leftAlign;
                var preAlignedRight = right + rightAlign - N;
        
                // Alignment with vectorization is tricky, so read carefully before changing code:
                // 1. We load data, which we might need to align, if the alignment hints
                //    mean pre-alignment (or overlapping alignment)
                // 2. We partition and store in the following order:
                //    a) right-portion of right vector to the right-side
                //    b) left-portion of left vector to the right side
                //    c) at this point one-half of each partitioned vector has been committed
                //       back to memory.
                //    d) we advance the right write (tmpRight) pointer by how many elements
                //       were actually needed to be written to the right hand side
                //    e) We write the right portion of the left vector to the right side
                //       now that its write position has been updated
        
                var RT0 = {self.get_load_vector('preAlignedRight')};
                var LT0 = {self.get_load_vector('preAlignedLeft')};
                {generate_comparison_operations(t)}
                var rtPopCountRightPart = Math.Max({self.get_popcnt('rtMask', 'int32')}, (uint)rightAlign);       
                var rtPopCountLeftPart = N - rtPopCountRightPart;
                var ltPopCountRightPart = {self.get_popcnt('ltMask', 'int32')}; // default(MT).popcnt(ltMask);
                var ltPopCountLeftPart = N - ltPopCountRightPart;
        
                { generate_permutations_with_compress() if compress_writes else generate_permutations_without_compress()}
            }}      
        """
        except:
            s = f""" {method_name} {{ throw new NotImplementedException(); }}"""

        print(s, file=f)

    def generate_configuration_struct(self, f):
        t = self.type
        s = f"""
        internal struct {self.capital_type_map[t]}Config
        {{
            public const int N = {self.vector_size()};
            
            public const int Unroll = { self.unroll };
            public const int SlackPerSideInVectors = Unroll;
            public const int SlackPerSideInElements = SlackPerSideInVectors * N;
            public const int SmallSortThresholdElements = {self.get_small_sort_threshold_elements('N')};
            
            // The formula for figuring out how much temporary space we need for partitioning:
            // 2 x the number of slack elements on each side for the purpose of partitioning in unrolled manner +
            // 2 x amount of maximal bytes needed for alignment (32)
            // one more vector's worth of elements since we write with N-way stores from both ends of the temporary area
            // and we must make sure we do not accidentally over-write from left -> right or vice-versa right on that edge...
            // In other words, while we allocated this much temp memory, the actual amount of elements inside said memory
            // is smaller by 8 elements + 1 for each alignment (max alignment is actually N-1, I just round up to N...)
            // This long sense just means that we over-allocate N+2 elements...
            public const int PartitionTempSizeInElements = (2 * SlackPerSideInElements + N + 4 * N);   
            public const int PartitionTempSizeInBytes = PartitionTempSizeInElements * sizeof({self.type});
            public const int ElementAlign = sizeof({t}) - 1;    
        }}
        """

        print(s, file=f)

    def generate_scalar_alignments(self, f):
        g = self
        t = self.type
        s = f"""
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private static {t}* align_left_scalar_uncommon({t}* read_left, {t} pivot, ref {t}* tmp_left, ref {t}* tmp_right)
        {{
            /// Called when the left hand side of the entire array does not have enough elements
            /// for us to align the memory with vectorized operations, so we do this uncommon slower alternative.
            /// Generally speaking this is probably called for all the partitioning calls on the left edge of the array
            
            if (((ulong)read_left & Sort.ALIGN_MASK) == 0)
                return read_left;

            var next_align = ({t}*)(((ulong)read_left + Sort.ALIGN) & ~Sort.ALIGN_MASK);
            while (read_left < next_align)
            {{
                var v = *(read_left++);
                if (v <= pivot)
                {{
                    *(tmp_left++) = v;
                }}
                else
                {{
                    *(--tmp_right) = v;
                }}
            }}

            return read_left;
        }}
        
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        private static {t}* align_right_scalar_uncommon({t}* readRight, {t} pivot, ref {t}* tmpLeft, ref {t}* tmpRight)
        {{        
            /// Called when the right hand side of the entire array does not have enough elements
            /// for us to align the memory with vectorized operations, so we do this uncommon slower alternative.
            /// Generally speaking this is probably called for all the partitioning calls on the right edge of the array
            
            if (((ulong)readRight & Sort.ALIGN_MASK) == 0)
                return readRight;

            var nextAlign = ({t}*)((ulong)readRight & ~Sort.ALIGN_MASK);
            while (readRight > nextAlign)
            {{
                var v = *(--readRight);
                if (v <= pivot)
                {{
                    *(tmpLeft++) = v;
                }}
                else
                {{
                    *(--tmpRight) = v;
                }}
            }}

            return readRight;
        }}         
"""
        print(s, file=f)

    def generate_load_and_partition_vectors(self, f, i):

        if i is None or i in self._generated_load_partition:
            return ""

        # Signal we wont generate it again.
        self._generated_load_partition.append(i)

        def get_unrolled_load(i):
            s = ""
            total = i
            for i in range(i, 0, -1):
                s += F"""
            var d{i} = {self.get_load_vector(f'({t}*)(dataPtr + N * {total - i})')};"""

            return s

        def get_unrolled_partition(i):
            s = ""
            for i in range(i, 0, -1):
                s += F"""
            partition_block(d{i}, P, pBase, ref writeLeftPtr, ref writeRightPtr);"""

            return s

        t = self.type

        method_name = f"""
        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static void LoadAndPartition{i}Vectors({t}* dataPtr, V P, byte* pBase, ref {t}* writeLeftPtr, ref {t}* writeRightPtr)"""

        try:
            s = f"""
        {method_name}
        {{
            // PERF: Unroll and CompressWrite support is been treated as a constant because we make sure the caller
            //       treats that parameter already as a constant @ JIT time causing a cascade.

            var N = V.Count; // Treated as constant @ JIT time

            { get_unrolled_load(i) }
            { get_unrolled_partition(i) }
        }}            
"""
        except:
            s = f""" {method_name} {{ throw new NotImplementedException(); }}"""

        print(s, file=f)

    def generate_if_debug(self, s):
        if self.is_debug:
            return s
        return ""

    def generate_if_prefetch(self, s):
        if self.do_prefetch:
            return s
        return ""

    def generate_vectorized_partition(self, f, i):

        def generate_unroll2(i):
            if i > 1:
                s = F"""
            int unrollHalf = {self.get_configuration_constant('Unroll')} / 2;
            readRightV += {self.get_configuration_constant("N")} * unrollHalf;
            while (readLeftV < readRightV)
            {{
                if ((byte*)writeRight - (byte*)readRightV < (2 * (unrollHalf * {self.get_configuration_constant("N")}) - {self.get_configuration_constant("N")}) * sizeof({t}))
                {{
                    { self.generate_if_prefetch(F"Sse.Prefetch0(readRightV);") }
                    { self.generate_if_prefetch(F"Sse.Prefetch1(readRightV - 2 * {self.get_configuration_constant('N')} * {i});") }
                    { self.generate_if_prefetch(F"Sse.Prefetch1(readRightV - 3 * {self.get_configuration_constant('N')} * {i});") }

                    nextPtr = readRightV;
                    readRightV -= {self.get_configuration_constant("N")} * unrollHalf;
                }}
                else
                {{
                    { self.generate_if_prefetch(F"Sse.Prefetch0(readLeftV);") }
                    { self.generate_if_prefetch(F"Sse.Prefetch1(readLeftV + 2 * {self.get_configuration_constant('N')} * {i});") }
                    { self.generate_if_prefetch(F"Sse.Prefetch1(readLeftV + 3 * {self.get_configuration_constant('N')} * {i});") }

                    // PERF: Ensure that JIT never emits cmov here.
                    nextPtr = readLeftV;
                    readLeftV += {self.get_configuration_constant("N")} * unrollHalf;
                }}

                LoadAndPartition{i}Vectors(nextPtr, P, pBase, ref writeLeft, ref writeRight);
            }}

            readRightV += {self.get_configuration_constant("N")} * (unrollHalf - 1);"""
            else:
                s = F"""
            readRightV += {self.get_configuration_constant("N")} * (unroll - 1);"""
            return s

        t = self.type

        method_name = f"""
        [SkipLocalsInit]
        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private {t}* vectorized_partition_{i}({t}* left, {t}* right, long hint)"""

        try:
            s = f"""
        {method_name}
        {{
            Debug.Assert(right - left >= {self.get_configuration_constant("SmallSortThresholdElements")});
            Debug.Assert(((long)left & {self.get_configuration_constant("ElementAlign")}) == 0);
            Debug.Assert(((long)right & {self.get_configuration_constant("ElementAlign")}) == 0);

            // Vectorized double-pumped (dual-sided) partitioning:
            // We start with picking a pivot using the media-of-3 "method"
            // Once we have sensible pivot stored as the last element of the array
            // We process the array from both ends.
            //
            // To get this rolling, we first read 2 Vector256 elements from the left and
            // another 2 from the right, and store them in some temporary space in order
            // to leave enough "space" inside the vector for storing partitioned values.
            // Why 2 from each side? Because we need n+1 from each side where n is the
            // number of Vector256 elements we process in each iteration... The
            // reasoning behind the +1 is because of the way we decide from *which* side
            // to read, we may end up reading up to one more vector from any given side
            // and writing it in its entirety to the opposite side (this becomes
            // slightly clearer when reading the code below...) Conceptually, the bulk
            // of the processing looks like this after clearing out some initial space
            // as described above:

            // [.............................................................................]
            //  ^wl          ^rl                                               rr^ wr^
            // Where:
            // wl = writeLeft
            // rl = readLeft
            // rr = readRight
            // wr = writeRight

            // In every iteration, we select what side to read from based on how much
            // space is left between head read/write pointer on each side...
            // We read from where there is a smaller gap, e.g. that side
            // that is closer to the unfortunate possibility of its write head
            // overwriting its read head... By reading from THAT side, we're ensuring
            // this does not happen

            // An additional unfortunate complexity we need to deal with is that the
            // right pointer must be decremented by another Vector256<T>.Count elements
            // Since the Load/Store primitives obviously accept start addresses
            var pivot = *right;

            // We do this here just in case we need to pre-align to the right
            // We end up
            *right = {t}.MaxValue;

            // Broadcast the selected pivot
            var P = {self.get_broadcast('pivot')};

            var readLeft = left;
            var readRight = right;

            {self.generate_if_debug(F'''Span<{t}> tempSpan = new Span<{t}>(({t}*)_tempPtr, {self.get_configuration_constant("PartitionTempSizeInElements")});''')}

            var tmpStartLeft = ({t}*)_tempPtr;
            var tmpLeft = tmpStartLeft;
            var tmpStartRight = tmpStartLeft + {self.get_configuration_constant("PartitionTempSizeInElements")};
            var tmpRight = tmpStartRight;

            tmpRight -= {self.get_configuration_constant("N")};

            {self.generate_if_debug(f'''Console.WriteLine($"Values:[{{string.Join(',', new Span<{t}>(left, (int)(right - left)).ToArray())}}]");''')}

            var leftAlign = unchecked((int)(hint & 0xFFFFFFFF));
            var rightAlign = unchecked((int)(hint >> 32));

            var pBase = {self.get_permutation_table_ptr()};

            // the read heads always advance by 8 elements, or 32 bytes,
            // We can spend some extra time here to align the pointers
            // so they start at a cache-line boundary
            // Once that happens, we can read with Avx.LoadAlignedVector256
            // And also know for sure that our reads will never cross cache-lines
            // Otherwise, 50% of our AVX2 Loads will need to read from two cache-lines
            align_vectorized(left, right,
                leftAlign, rightAlign, P, pBase,
                ref readLeft, ref readRight,
                ref tmpStartLeft, ref tmpLeft, ref tmpStartRight, ref tmpRight);

            if (leftAlign > 0)
            {{
                tmpRight += {self.get_configuration_constant("N")};
                readLeft = align_left_scalar_uncommon(readLeft, pivot, ref tmpLeft, ref tmpRight);
                tmpRight -= {self.get_configuration_constant("N")};
            }}

            if (rightAlign < 0)
            {{
                tmpRight += {self.get_configuration_constant("N")};
                readRight = align_right_scalar_uncommon(readRight, pivot, ref tmpLeft, ref tmpRight);
                tmpRight -= {self.get_configuration_constant("N")};
            }}

            Debug.Assert(((ulong)readLeft & Sort.ALIGN_MASK) == 0);
            Debug.Assert(((ulong)readRight & Sort.ALIGN_MASK) == 0);

            Debug.Assert((((ulong)readRight - (ulong)readLeft) % Sort.ALIGN) == 0);
            Debug.Assert((readRight - readLeft) >= {self.get_configuration_constant("Unroll")} * 2);

            // From now on, we are fully aligned
            // and all reading is done in full vector units

            var readLeftV = readLeft;
            var readRightV = readRight;            

            // PERF: This diminished the size of the method and improves the performance. 
            var pointers = stackalloc {t}*[2];
            pointers[0] = readLeftV;
            pointers[1] = readRightV - {self.get_configuration_constant("Unroll")} * {self.get_configuration_constant("N")};
            
            { self.generate_if_prefetch("Sse.Prefetch0(pointers[0]);") }
            { self.generate_if_prefetch("Sse.Prefetch0(pointers[1]);") }
            
            for ( int i = 0; i < 2; i++)
                LoadAndPartition{i}Vectors(pointers[i], P, pBase, ref tmpLeft, ref tmpRight);

            tmpRight += {self.get_configuration_constant("N")};

            {self.generate_if_debug(f'''Console.WriteLine($"TempL:[{{string.Join(',', new Span<{t}>(tmpStartLeft, (int)(tmpLeft - tmpStartLeft)).ToArray())}}]");''')}
            {self.generate_if_debug(f'''Console.WriteLine($"TempR:[{{string.Join(',', new Span<{t}>(tmpRight, (int)(tmpStartRight - tmpRight)).ToArray())}}]");''')}

            // Adjust for the reading that was made above
            readLeftV += {self.get_configuration_constant("N")} * {self.get_configuration_constant("Unroll")};
            readRightV -= {self.get_configuration_constant("N")} * {self.get_configuration_constant("Unroll")} * 2;

            {t}* nextPtr;

            var writeLeft = left;
            var writeRight = right - {self.get_configuration_constant("N")};

            while (readLeftV < readRightV)
            {{
                if ((byte*)writeRight - (byte*)readRightV < (2 * ({self.get_configuration_constant("Unroll")} * {self.get_configuration_constant("N")}) - {self.get_configuration_constant("N")}) * sizeof({t}))
                {{
                    { self.generate_if_prefetch(F"Sse.Prefetch0(readRightV);") }
                    { self.generate_if_prefetch(F"Sse.Prefetch1(readRightV - 2 * {self.get_configuration_constant('N')} * {self.unroll});") }
                    { self.generate_if_prefetch(F"Sse.Prefetch1(readRightV - 3 * {self.get_configuration_constant('N')} * {self.unroll});") }
                    
                    nextPtr = readRightV;
                    readRightV -= {self.get_configuration_constant("N")} * {self.get_configuration_constant("Unroll")};
                }}
                else
                {{
                    { self.generate_if_prefetch(F"Sse.Prefetch0(readLeftV);") }
                    { self.generate_if_prefetch(F"Sse.Prefetch1(readLeftV + 2 * {self.get_configuration_constant('N')} * {self.unroll});") }
                    { self.generate_if_prefetch(F"Sse.Prefetch1(readLeftV + 3 * {self.get_configuration_constant('N')} * {self.unroll});") }
                    
                    // PERF: Ensure that JIT never emits cmov here.
                    nextPtr = readLeftV;
                    readLeftV += {self.get_configuration_constant("N")} * {self.get_configuration_constant("Unroll")};
                }}

                LoadAndPartition{i}Vectors(nextPtr, P, pBase, ref writeLeft, ref writeRight);
            }}
            
            { generate_unroll2(self.unroll2) }                                                

            {self.generate_if_debug(F'''Console.WriteLine($"WL:[{{string.Join(',', new Span<{t}>(left, (int)(writeLeft - left)).ToArray())}}]");''')}
            {self.generate_if_debug(F'''Console.WriteLine($"WR:[{{string.Join(',', new Span<{t}>(writeRight, (int)(right - writeRight)).ToArray())}}]");''')}

            while (readLeftV <= readRightV)
            {{
                if ((byte*)writeRight - (byte*)readRightV < {self.get_configuration_constant("N")} * sizeof({t}))
                {{
                    nextPtr = readRightV;
                    readRightV -= {self.get_configuration_constant("N")};                                
                }}
                else
                {{                   
                    // PERF: Ensure that JIT never emits cmov here.
                    nextPtr = readLeftV;
                    readLeftV += {self.get_configuration_constant("N")};                    
                }}

                LoadAndPartition{1}Vectors(nextPtr, P, pBase, ref writeLeft, ref writeRight);
            }}

            {self.generate_if_debug(f'''Console.WriteLine($"WL:[{{string.Join(',', new Span<{t}>(left, (int)(writeLeft - left)).ToArray())}}]");''')}
            {self.generate_if_debug(f'''Console.WriteLine($"WR:[{{string.Join(',', new Span<{t}>(writeRight, (int)(right - writeRight)).ToArray())}}]");''')}

            // 3. Copy-back the 4 registers + remainder we partitioned in the beginning
          
            var leftTmpSize = tmpLeft - tmpStartLeft;
            Unsafe.CopyBlockUnaligned(writeLeft, tmpStartLeft, (uint)(leftTmpSize * sizeof({t})));
            writeLeft += leftTmpSize;

            {self.generate_if_debug(f'''Console.WriteLine($"WL:[{{string.Join(',', new Span<{t}>(left, (int)(writeLeft - left)).ToArray())}}]");''')}
            var rightTmpSize = tmpStartRight - tmpRight;
            Unsafe.CopyBlockUnaligned(writeLeft, tmpRight, (uint)(rightTmpSize * sizeof({t})));            
           
            {self.generate_if_debug(f'''Console.WriteLine($"W:[{{string.Join(',', new Span<{t}>(left, (int)(right - left)).ToArray())}}]");''')}
            // Shove to pivot back to the boundary
            *right = *writeLeft;
            *writeLeft++ = pivot;

            {self.generate_if_debug(f'''Console.WriteLine($"W:[{{string.Join(',', new Span<{t}>(left, (int)(right - left)).ToArray())}}]");''')}

            Debug.Assert(writeLeft > left);
            Debug.Assert(writeLeft <= right + 1);

            return writeLeft;
        }}            
"""
        except:
            s = f""" {method_name} {{ throw new NotImplementedException(); }}"""

        print(s, file=f)

    def generate_sort_primitives(self, f):
        t = self.type
        s = f"""
        private static void Swap({t}* left, {t}* right)
        {{
            var tmp = *left;
            *left = *right;
            *right = tmp;
        }}    
        
        private static unsafe void SwapIfGreater({t}* leftPtr, {t}* rightPtr)            
        {{
            if (*leftPtr <= *rightPtr) return;
            Swap(leftPtr, rightPtr);
        }}
        
        private static unsafe void SwapIfGreater3(in {t}* leftPtr, in {t}* middlePtr, in {t}* rightPtr)
        {{
            SwapIfGreater(leftPtr, middlePtr);
            SwapIfGreater(leftPtr, rightPtr);
            SwapIfGreater(middlePtr, rightPtr);
        }}
              
        [MethodImpl(MethodImplOptions.AggressiveInlining)]
        static void down_heap(long i, long n, {t}* lo)
        {{
            var d = *(lo + i - 1);
            long child;
            while (i <= n / 2)
            {{
                child = 2 * i;
                if (child < n && *(lo + child - 1) < *(lo + child))
                {{
                    child++;
                }}
                if (!(d < *(lo + child - 1)))
                {{
                    break;
                }}
                *(lo + i - 1) = *(lo + child - 1);
                i = child;
            }}
            *(lo + i - 1) = d;
        }}        
        
        [MethodImpl(MethodImplOptions.AggressiveOptimization)]
        static void heap_sort({t}* lo, {t}* hi)
        {{
            long n = hi - lo + 1;
            for (long i = n / 2; i >= 1; i--)
            {{
                down_heap(i, n, lo);
            }}
            for (long i = n; i > 1; i--)
            {{
                Swap(lo, lo + i - 1);
                down_heap(1, i - 1, lo);
            }}
        }}   
"""
        print(s, file=f)

    def generate_try_pack(self):

        t = self.type
        if self.sorting_type_map[t] == 'UInt32' or self.sorting_type_map[t] == 'Int32' or t == 'double' or not self.can_pack:
            return ""
        tt = self.pack_type_map[self.type]

        return F"""    
            if (({self.unsigned_type_map[t]}) (right_hint - left_hint) < {self.unsigned_type_map[tt]}.MaxValue << {self.shift})
            {{
                {self.generate_if_debug(F'''Console.WriteLine($"Pre:Unpacked[{{string.Join(',', new Span<{t}>(({t}*)left, length).ToArray())}}]");''')}
                Pack(left, length, left_hint);
                {self.generate_if_debug(F'''Console.WriteLine($"Pre:Packed[{{string.Join(',', new Span<{tt}>(({tt}*)left, length).ToArray())}}]");''')}
                
                {tt}* il = ({tt}*) left;
                {tt}* ir = il + (length - 1);
                var sorter = new Avx2VectorizedSort(il, ir, _tempPtr, _tempLength);
                sorter.sort(il, ir, 0, 0, Sort.REALIGN_BOTH, depth_limit);
                
                {self.generate_if_debug(F'''Console.WriteLine($"Post:Packed[{{string.Join(',', new Span<{tt}>(({tt}*)left, length).ToArray())}}]");''')}
                Unpack(left, length, left_hint);
                {self.generate_if_debug(F'''Console.WriteLine($"Post:Unpacked[{{string.Join(',', new Span<{t}>(({t}*)left, length).ToArray())}}]");''')}
                return;
            }}"""

    def generate_pack(self, f):

        def generate_pack_vectorized(tto, type_map, shift, respect_packing_order):

            if respect_packing_order:
                raise Exception("NotSupported")

            packing_instruction = F"""
            byte _MM_PERM_CDAB = 0xB1;
            var di1 = d1.AsInt32();
            var di2 = d2.AsInt32();
            di2 = Shuffle(di2, _MM_PERM_CDAB);
            return Blend(di1, di2, 0b10101010).As{type_map}();
"""

            s = F"""        
        [SkipLocalsInit]
        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static Vector256<{tto}> pack_vectorized(V baseVec, V d1, V d2)
        {{
            byte Shift = {shift};
                        
            if (Shift > 0) 
            {{ 
                // JIT will compile it in/out based on the constant value of Shift
                d1 = ShiftRightLogical(d1, Shift);
                d2 = ShiftRightLogical(d2, Shift);
            }}
            
            d1 = Subtract(d1, baseVec);
            d2 = Subtract(d2, baseVec);            
            {packing_instruction}
        }}"""
            return s

        def generate_unroll(i):
            s = ""
            for r in range(i):
                idx = r * 2
                s += F"""
                var d{idx} = {self.get_load_vector(f'({t}*)(memv_read + NFrom * {idx})')};
                var d{idx + 1} = {self.get_load_vector(f'({t}*)(memv_read + NFrom * {idx + 1})')};                                
"""
            s += "\n                "
            for r in range(i):
                idx = r * 2
                s += F"""{self.get_store_vec(F'''memv_write + NTo * {r}''', F'''pack_vectorized(baseVec, d{idx}, d{idx+1})''')};                
                """
            return s

        t = self.type
        if self.sorting_type_map[t] == 'UInt32' or self.sorting_type_map[t] == 'Int32' or t == 'double':
            return

        tt = self.pack_type_map[t]
        s = F"""
        
        {generate_pack_vectorized(tt, self.sorting_type_map[tt], self.shift, False)}
        
        private static void Pack(void* mem, int len, {t} @base)
        {{
            var NFrom = V.Count;
            var NTo = Vector256<{tt}>.Count;
            var Unroll = {self.pack_unroll};
        
            var offset = {self.get_shift_n_sub(self.shift, '@base', f'''{tt}.MinValue''') };
            var baseVec = {self.get_broadcast('offset')};
            
            var pre_aligned_mem = ({t}*) ( (ulong)mem & ~Sort.ALIGN_MASK );
            var mem_read = ({t}*) mem;
            var mem_write = ({tt}*) mem;
            
            if ( len < NFrom )
            {{
                while ( len > 0 )
                {{
                    len--;
                    
                    *mem_write = ({tt})( {self.get_shift_n_sub(self.shift, '*mem_read', "offset") } );
                    mem_read++;
                    mem_write++;
                }}
                return;
            }}
                    
            // We have at least
            // one vector worth of data to handle
            // Let's try to align to vector size first
            
            if (pre_aligned_mem < mem) 
            {{
                var alignment_point = pre_aligned_mem + NFrom;
                len -= (int) (alignment_point - mem_read);
                while (mem_read < alignment_point) 
                {{
                    *mem_write = ({tt})( {self.get_shift_n_sub(self.shift, '*mem_read', "offset") } );
                    mem_read++;
                    mem_write++;
                }}
            }}
            
            Debug.Assert(((ulong)(mem_read) & Sort.ALIGN_MASK) == 0);
            
            var memv_read = mem_read;
            var memv_write = mem_write;
            
            var lenv = len / NFrom;
            len -= (lenv * NFrom);
            while (lenv >= 2 * Unroll) 
            {{
                Debug.Assert( memv_read >= memv_write);                
                { generate_unroll(self.pack_unroll) }                
                memv_read += Unroll * 2 * NFrom;
                memv_write += Unroll * NTo;
                lenv -= 2 * Unroll;
            }}
            
            if ( Unroll > 1 )
            {{                                
                while (lenv >= 2) 
                {{
                    Debug.Assert( memv_read >= memv_write );
    
                    var d01 = {self.get_load_vector(f'memv_read + NFrom * 0')}; 
                    var d02 = {self.get_load_vector(f'memv_read + NFrom * 1')};
                    {self.get_store_vec("memv_write + NTo * 0 ", "pack_vectorized(baseVec, d01, d02)")};
                                        
                    memv_read += 2 * NFrom;
                    memv_write += NTo;
                    lenv -= 2;
                }}
            }}
            
            len += lenv * NFrom;

            mem_read = memv_read;
            mem_write = memv_write;
            
            while (len-- > 0) 
            {{
                *mem_write = ({tt})( {self.get_shift_n_sub(self.shift, '*mem_read', "offset") } );
                mem_read++;
                mem_write++;
            }}
        }} 
        """
        print(s, file=f)

    def generate_unpack(self, f):

        t = self.type
        if self.sorting_type_map[t] == 'UInt32' or self.sorting_type_map[t] == 'Int32' or t == 'double':
            return

        def generate_unpack_vectorized(tto, type_map, shift, respect_packing_order):

            if respect_packing_order:
                raise Exception("NotSupported")

            unpacking_instruction = F"""
            u0 = ConvertToVector256Int64(ExtractVector128(d0, 0)).As{type_map}();
            u1 = ConvertToVector256Int64(ExtractVector128(d0, 1)).As{type_map}();               
    """

            s = F"""        
        [SkipLocalsInit]
        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        private static void unpack_vectorized(V baseVec, Vector256<{tto}> d0, out V u0, out V u1)
        {{
            byte Shift = {shift};

            {unpacking_instruction}    
            
            u0 = Add(u0, baseVec);
            u1 = Add(u1, baseVec);
            
            if (Shift > 0)
            {{
                u0 = ShiftLeftLogical(u0, Shift);
                u1 = ShiftLeftLogical(u1, Shift);
            }}     
        }}"""
            return s

        def generate_unroll(i):
            s = ""
            for r in range(i):
                s += F"""
                var d{r} = {self.get_load_vector(f'({tt}*) (memv_read - NTo * {r})', aligned=False)};"""
            s += "\n                "
            for r in range(i):
                idx = r * 2
                s += F"""
                unpack_vectorized(baseVec, d{r}, out var u{idx}, out var u{idx + 1});
                {self.get_store_vec(F'''memv_write - NTo * {r} + NFrom * 0''', F'''u{idx}''')};
                {self.get_store_vec(F'''memv_write - NTo * {r} + NFrom * 1''', F'''u{idx+1}''')};                
                    """
            return s

        tt = self.pack_type_map[t]
        s = F"""
        
        {generate_unpack_vectorized(tt, self.sorting_type_map[t], self.shift, False)}
                
        private static void Unpack(void* mem, int len, {t} @base)
        {{
            var NFrom = V.Count;
            var NTo = Vector256<{tt}>.Count;
            var Unroll = {self.pack_unroll};
        
            var offset = {self.get_shift_n_sub(self.shift, '@base', f'''{tt}.MinValue''') };
            var baseVec = {self.get_broadcast('offset')};
        
            var mem_read = (({tt}*) mem) + len;
            var mem_write = (({t}*) mem) + len;
            
            if ( len < NTo )
            {{
                while ( len-- != 0 )
                {{                
                    *(--mem_write) = {self.get_unshift_n_add(self.shift, '*(--mem_read)', "offset")};                    
                }}
                return;
            }}
            
            var pre_aligned_mem = ({tt}*) ( (ulong)mem_read & ~Sort.ALIGN_MASK );           
            if (pre_aligned_mem < mem_read) 
            {{
                len -= (int)(mem_read - pre_aligned_mem);
                while (mem_read > pre_aligned_mem) 
                {{
                   *(--mem_write) = {self.get_unshift_n_add(self.shift, '*(--mem_read)', "offset")};
                }}
            }}
            
            Debug.Assert(((ulong)(mem_read) & Sort.ALIGN_MASK) == 0);
            
            var lenv = len / NTo;           
            len -= lenv * NTo;
                        
            var memv_read = mem_read - NTo;
            var memv_write = mem_write - 2 * NFrom;            
            while (lenv >= Unroll) 
            {{
                Debug.Assert( memv_read <= memv_write);                                
                { generate_unroll(self.pack_unroll) }                 
                memv_read -= Unroll * NTo;
                memv_write -= Unroll * (2 * NFrom);
                lenv -= Unroll;
            }}
            
            if ( Unroll > 1 )
            {{                                
                while (lenv >= 1) 
                {{
                    Debug.Assert( memv_read <= memv_write );
    
                    var d01 = {self.get_load_vector(f'memv_read', aligned=False)}; 
                    
                    unpack_vectorized(baseVec, d01, out var u01, out var u02);
                    {self.get_store_vec("memv_write + NFrom * 0", "u01")};
                    {self.get_store_vec("memv_write + NFrom * 1", "u02")};
                                        
                    memv_read -= NTo;
                    memv_write -= 2 * NFrom;
                    lenv -= 1;
                }}
            }}
            
            mem_read = memv_read + NTo;
            mem_write = memv_write + 2 * NFrom;
            
            while (len-- > 0) 
            {{
                *(--mem_write) = {self.get_unshift_n_add(self.shift, '*(--mem_read)', "offset")};
            }}                   
        }}        
        """
        print(s, file=f)

    def generate_sort(self, f):
        t = self.type
        s = f"""
        internal void sort({t}* left, {t}* right, {t} left_hint, {t} right_hint, long hint, int depth_limit)
        {{
            var length = (int)(right - left + 1);

            {t}* mid;
            switch (length)
            {{
                case 0:
                case 1:
                    return;
                case 2:
                    SwapIfGreater(left, right);
                    return;
                case 3:
                    mid = right - 1;
                    SwapIfGreater(left, mid);
                    SwapIfGreater(left, right);
                    SwapIfGreater(mid, right);
                    return;
            }}

            // Go to insertion sort below this threshold
            if (length <= {self.get_configuration_constant("SmallSortThresholdElements")})
            {{
                {self.generate_if_debug(F'''var idx = left - ({t}*)_startPtr;''')}
                {self.generate_if_debug('''Console.WriteLine($"B({depth_limit}):[{idx}|{idx + length}]");''')}
                BitonicSort.Sort(left, length);
                return;
            }}

            // Detect a whole bunch of bad cases where partitioning
            // will not do well:
            // 1. Reverse sorted array
            // 2. High degree of repeated values (dutch flag problem, one value)
            if (depth_limit == 0)
            {{
                heap_sort(left, right);
                {self.generate_if_debug('_depth--;')}
                return;
            }}

            depth_limit--;
            { self.generate_try_pack() }
            { self.generate_if_debug(F'''var values = new Span<{t}>(({t}*)_startPtr, (int)(({t}*)_endPtr - ({t}*)_startPtr));''') }
            { self.generate_if_debug('''Console.WriteLine($"Start({depth_limit})=[{string.Join(',', values.ToArray())}]");''') }

            // This is going to be a bit weird:
            // Pre/Post alignment calculations happen here: we prepare hints to the
            // partition function of how much to align and in which direction (pre/post).
            // The motivation to do these calculations here and the actual alignment inside the partitioning code is
            // that here, we can cache those calculations.
            // As we recurse to the left we can reuse the left cached calculation, And when we recurse
            // to the right we reuse the right calculation, so we can avoid re-calculating the same aligned addresses
            // throughout the recursion, at the cost of a minor code complexity
            // Since we branch on the magi values REALIGN_LEFT & REALIGN_RIGHT its safe to assume
            // the we are not torturing the branch predictor.'

            // We use a long as a "struct" to pass on alignment hints to the partitioning
            // By packing 2 32 bit elements into it, as the JIT seem to not do this.
            // In reality  we need more like 2x 4bits for each side, but I don't think
            // there is a real difference'

            var preAlignedLeft = ({t}*)((ulong)left & ~Sort.ALIGN_MASK);
            var cannotPreAlignLeft = ((long)preAlignedLeft - (long)_startPtr) >> 63;
            var preAlignLeftOffset = (preAlignedLeft - left) + ({self.get_configuration_constant("N")} & cannotPreAlignLeft);
            if ((hint & Sort.REALIGN_LEFT) != 0)
            {{
                // Alignment flow:
                // * Calculate pre-alignment on the left
                // * See it would cause us an out-of bounds read
                // * Since we'd like to avoid that, we adjust for post-alignment
                // * There are no branches since we do branch->arithmetic
                hint &= unchecked((long)0xFFFFFFFF00000000UL);
                hint |= preAlignLeftOffset;
            }}

            var preAlignedRight = ({t}*)(((ulong)right - 1 & ~Sort.ALIGN_MASK) + Sort.ALIGN);
            var cannotPreAlignRight = ((long)_endPtr - (long)preAlignedRight) >> 63;
            var preAlignRightOffset = (preAlignedRight - right - ({self.get_configuration_constant("N")} & cannotPreAlignRight));
            if ((hint & Sort.REALIGN_RIGHT) != 0)
            {{
                // right is pointing just PAST the last element we intend to partition (where we also store the pivot)
                // So we calculate alignment based on right - 1, and YES: I am casting to ulong before doing the -1, this
                // is intentional since the whole thing is either aligned to 32 bytes or not, so decrementing the POINTER value
                // by 1 is sufficient for the alignment, an the JIT sucks at this anyway
                hint &= 0xFFFFFFFF;
                hint |= preAlignRightOffset << 32;
            }}

            Debug.Assert(((ulong)(left + (hint & 0xFFFFFFFF)) & Sort.ALIGN_MASK) == 0);
            Debug.Assert(((ulong)(right + (hint >> 32)) & Sort.ALIGN_MASK) == 0);

            // Compute median-of-three, of:
            // the first, mid and one before last elements
            mid = left + ((right - left) / 2);
            SwapIfGreater3(left, mid, right - 1);

            // Pivot is mid, place it in the right hand side
            Swap(mid, right);

            {self.generate_if_debug(F'''Console.WriteLine($"V({{depth_limit}}):[{{(left - ({t}*)_startPtr)}}|{{ (right - ({t}*)_startPtr)}}]");''')}
            {self.generate_efficient_partitioning_call('sep')}
            {self.generate_if_debug(F'''Console.WriteLine($"Sep({{depth_limit}}):[{{(sep - 1 - ({t}*)_startPtr)}}|{{*(sep - 1)}}]");''')}

            {self.generate_if_debug('_depth++;')}
            {self.generate_if_debug(F'''Console.WriteLine($"L({{depth_limit}}):[{{(left - ({t}*)_startPtr)}}|{{ (sep - 2 - ({t}*)_startPtr)}}]");''')}
            sort(left, sep - 2, left_hint, *sep, hint | Sort.REALIGN_RIGHT, depth_limit);
            {self.generate_if_debug(F'''Console.WriteLine($"R({{depth_limit}}):[{{(sep - ({t}*)_startPtr)}}|{{ (right - ({t}*)_startPtr)}}]");''')}
            sort(sep, right, *(sep - 2), right_hint, hint | Sort.REALIGN_LEFT, depth_limit);
            {self.generate_if_debug('_depth--;')}

            {self.generate_if_debug('''Console.WriteLine($"End({depth_limit})=[{string.Join(',', values.ToArray())}]");''')}
        }}
"""
        print(s, file=f)

    def generate_master_entry_point(self, f):
        t = self.type
        s = f"""
    internal unsafe partial struct Avx2VectorizedSort
    {{
        { self.generate_if_debug('private int _depth;') }
        private void* _startPtr;
        private void* _endPtr;    
        private readonly byte* _tempPtr;
        private readonly int _tempLength;

        [MethodImpl(MethodImplOptions.AggressiveInlining | MethodImplOptions.AggressiveOptimization)]
        public Avx2VectorizedSort(void* start, void* end, byte* buffer, int bufferSize)
        {{
            _startPtr   = start;
            _endPtr     = end;
            _tempPtr    = buffer;
            _tempLength = bufferSize;
            { self.generate_if_debug('_depth = 0;') }
        }} 
        
        // We might read the last 8 bytes into a 128-bit vector for the purpose of permutation
        // Most compilers actually fuse the pair of instructions: _mm256_cvtepi8_epiNN + _mm_loadu_si128
        // into a single instruction that will not read more that 4/8 bytes..
        // vpmovsxb[dq] ymm0, dword [...], eliminating the 128-bit load completely and effectively
        // reading exactly 4/8 (depending if the instruction is vpmovsxbd or vpmovsxbq)
        // without generating an out of bounds read at all.
        // But, life is harsh, and we can't trust the compiler to do the right thing if it is not
        // contractual
        
        internal static ReadOnlySpan<byte> perm_table_64 => new byte[]
        {{
            0, 1, 2, 3, 4, 5, 6, 7,  // 0b0000 (0)
            2, 3, 4, 5, 6, 7, 0, 1,  // 0b0001 (1)
            0, 1, 4, 5, 6, 7, 2, 3,  // 0b0010 (2)
            4, 5, 6, 7, 0, 1, 2, 3,  // 0b0011 (3)
            0, 1, 2, 3, 6, 7, 4, 5,  // 0b0100 (4)
            2, 3, 6, 7, 0, 1, 4, 5,  // 0b0101 (5)
            0, 1, 6, 7, 2, 3, 4, 5,  // 0b0110 (6)
            6, 7, 0, 1, 2, 3, 4, 5,  // 0b0111 (7)
            0, 1, 2, 3, 4, 5, 6, 7,  // 0b1000 (8)
            2, 3, 4, 5, 0, 1, 6, 7,  // 0b1001 (9)
            0, 1, 4, 5, 2, 3, 6, 7,  // 0b1010 (10)
            4, 5, 0, 1, 2, 3, 6, 7,  // 0b1011 (11)
            0, 1, 2, 3, 4, 5, 6, 7,  // 0b1100 (12)
            2, 3, 0, 1, 4, 5, 6, 7,  // 0b1101 (13)
            0, 1, 2, 3, 4, 5, 6, 7,  // 0b1110 (14)
            0, 1, 2, 3, 4, 5, 6, 7,  // 0b1111 (15)
            0, 0, 0, 0, 0, 0, 0, 0,  // Ensuring we cannot overrun the buffer.
        }};

        internal static ReadOnlySpan<byte> perm_table_32 => new byte[]
        {{
            0, 1, 2, 3, 4, 5, 6, 7, // 0b00000000 (0)
            1, 2, 3, 4, 5, 6, 7, 0, // 0b00000001 (1)
            0, 2, 3, 4, 5, 6, 7, 1, // 0b00000010 (2)
            2, 3, 4, 5, 6, 7, 0, 1, // 0b00000011 (3)
            0, 1, 3, 4, 5, 6, 7, 2, // 0b00000100 (4)
            1, 3, 4, 5, 6, 7, 0, 2, // 0b00000101 (5)
            0, 3, 4, 5, 6, 7, 1, 2, // 0b00000110 (6)
            3, 4, 5, 6, 7, 0, 1, 2, // 0b00000111 (7)
            0, 1, 2, 4, 5, 6, 7, 3, // 0b00001000 (8)
            1, 2, 4, 5, 6, 7, 0, 3, // 0b00001001 (9)
            0, 2, 4, 5, 6, 7, 1, 3, // 0b00001010 (10)
            2, 4, 5, 6, 7, 0, 1, 3, // 0b00001011 (11)
            0, 1, 4, 5, 6, 7, 2, 3, // 0b00001100 (12)
            1, 4, 5, 6, 7, 0, 2, 3, // 0b00001101 (13)
            0, 4, 5, 6, 7, 1, 2, 3, // 0b00001110 (14)
            4, 5, 6, 7, 0, 1, 2, 3, // 0b00001111 (15)
            0, 1, 2, 3, 5, 6, 7, 4, // 0b00010000 (16)
            1, 2, 3, 5, 6, 7, 0, 4, // 0b00010001 (17)
            0, 2, 3, 5, 6, 7, 1, 4, // 0b00010010 (18)
            2, 3, 5, 6, 7, 0, 1, 4, // 0b00010011 (19)
            0, 1, 3, 5, 6, 7, 2, 4, // 0b00010100 (20)
            1, 3, 5, 6, 7, 0, 2, 4, // 0b00010101 (21)
            0, 3, 5, 6, 7, 1, 2, 4, // 0b00010110 (22)
            3, 5, 6, 7, 0, 1, 2, 4, // 0b00010111 (23)
            0, 1, 2, 5, 6, 7, 3, 4, // 0b00011000 (24)
            1, 2, 5, 6, 7, 0, 3, 4, // 0b00011001 (25)
            0, 2, 5, 6, 7, 1, 3, 4, // 0b00011010 (26)
            2, 5, 6, 7, 0, 1, 3, 4, // 0b00011011 (27)
            0, 1, 5, 6, 7, 2, 3, 4, // 0b00011100 (28)
            1, 5, 6, 7, 0, 2, 3, 4, // 0b00011101 (29)
            0, 5, 6, 7, 1, 2, 3, 4, // 0b00011110 (30)
            5, 6, 7, 0, 1, 2, 3, 4, // 0b00011111 (31)
            0, 1, 2, 3, 4, 6, 7, 5, // 0b00100000 (32)
            1, 2, 3, 4, 6, 7, 0, 5, // 0b00100001 (33)
            0, 2, 3, 4, 6, 7, 1, 5, // 0b00100010 (34)
            2, 3, 4, 6, 7, 0, 1, 5, // 0b00100011 (35)
            0, 1, 3, 4, 6, 7, 2, 5, // 0b00100100 (36)
            1, 3, 4, 6, 7, 0, 2, 5, // 0b00100101 (37)
            0, 3, 4, 6, 7, 1, 2, 5, // 0b00100110 (38)
            3, 4, 6, 7, 0, 1, 2, 5, // 0b00100111 (39)
            0, 1, 2, 4, 6, 7, 3, 5, // 0b00101000 (40)
            1, 2, 4, 6, 7, 0, 3, 5, // 0b00101001 (41)
            0, 2, 4, 6, 7, 1, 3, 5, // 0b00101010 (42)
            2, 4, 6, 7, 0, 1, 3, 5, // 0b00101011 (43)
            0, 1, 4, 6, 7, 2, 3, 5, // 0b00101100 (44)
            1, 4, 6, 7, 0, 2, 3, 5, // 0b00101101 (45)
            0, 4, 6, 7, 1, 2, 3, 5, // 0b00101110 (46)
            4, 6, 7, 0, 1, 2, 3, 5, // 0b00101111 (47)
            0, 1, 2, 3, 6, 7, 4, 5, // 0b00110000 (48)
            1, 2, 3, 6, 7, 0, 4, 5, // 0b00110001 (49)
            0, 2, 3, 6, 7, 1, 4, 5, // 0b00110010 (50)
            2, 3, 6, 7, 0, 1, 4, 5, // 0b00110011 (51)
            0, 1, 3, 6, 7, 2, 4, 5, // 0b00110100 (52)
            1, 3, 6, 7, 0, 2, 4, 5, // 0b00110101 (53)
            0, 3, 6, 7, 1, 2, 4, 5, // 0b00110110 (54)
            3, 6, 7, 0, 1, 2, 4, 5, // 0b00110111 (55)
            0, 1, 2, 6, 7, 3, 4, 5, // 0b00111000 (56)
            1, 2, 6, 7, 0, 3, 4, 5, // 0b00111001 (57)
            0, 2, 6, 7, 1, 3, 4, 5, // 0b00111010 (58)
            2, 6, 7, 0, 1, 3, 4, 5, // 0b00111011 (59)
            0, 1, 6, 7, 2, 3, 4, 5, // 0b00111100 (60)
            1, 6, 7, 0, 2, 3, 4, 5, // 0b00111101 (61)
            0, 6, 7, 1, 2, 3, 4, 5, // 0b00111110 (62)
            6, 7, 0, 1, 2, 3, 4, 5, // 0b00111111 (63)
            0, 1, 2, 3, 4, 5, 7, 6, // 0b01000000 (64)
            1, 2, 3, 4, 5, 7, 0, 6, // 0b01000001 (65)
            0, 2, 3, 4, 5, 7, 1, 6, // 0b01000010 (66)
            2, 3, 4, 5, 7, 0, 1, 6, // 0b01000011 (67)
            0, 1, 3, 4, 5, 7, 2, 6, // 0b01000100 (68)
            1, 3, 4, 5, 7, 0, 2, 6, // 0b01000101 (69)
            0, 3, 4, 5, 7, 1, 2, 6, // 0b01000110 (70)
            3, 4, 5, 7, 0, 1, 2, 6, // 0b01000111 (71)
            0, 1, 2, 4, 5, 7, 3, 6, // 0b01001000 (72)
            1, 2, 4, 5, 7, 0, 3, 6, // 0b01001001 (73)
            0, 2, 4, 5, 7, 1, 3, 6, // 0b01001010 (74)
            2, 4, 5, 7, 0, 1, 3, 6, // 0b01001011 (75)
            0, 1, 4, 5, 7, 2, 3, 6, // 0b01001100 (76)
            1, 4, 5, 7, 0, 2, 3, 6, // 0b01001101 (77)
            0, 4, 5, 7, 1, 2, 3, 6, // 0b01001110 (78)
            4, 5, 7, 0, 1, 2, 3, 6, // 0b01001111 (79)
            0, 1, 2, 3, 5, 7, 4, 6, // 0b01010000 (80)
            1, 2, 3, 5, 7, 0, 4, 6, // 0b01010001 (81)
            0, 2, 3, 5, 7, 1, 4, 6, // 0b01010010 (82)
            2, 3, 5, 7, 0, 1, 4, 6, // 0b01010011 (83)
            0, 1, 3, 5, 7, 2, 4, 6, // 0b01010100 (84)
            1, 3, 5, 7, 0, 2, 4, 6, // 0b01010101 (85)
            0, 3, 5, 7, 1, 2, 4, 6, // 0b01010110 (86)
            3, 5, 7, 0, 1, 2, 4, 6, // 0b01010111 (87)
            0, 1, 2, 5, 7, 3, 4, 6, // 0b01011000 (88)
            1, 2, 5, 7, 0, 3, 4, 6, // 0b01011001 (89)
            0, 2, 5, 7, 1, 3, 4, 6, // 0b01011010 (90)
            2, 5, 7, 0, 1, 3, 4, 6, // 0b01011011 (91)
            0, 1, 5, 7, 2, 3, 4, 6, // 0b01011100 (92)
            1, 5, 7, 0, 2, 3, 4, 6, // 0b01011101 (93)
            0, 5, 7, 1, 2, 3, 4, 6, // 0b01011110 (94)
            5, 7, 0, 1, 2, 3, 4, 6, // 0b01011111 (95)
            0, 1, 2, 3, 4, 7, 5, 6, // 0b01100000 (96)
            1, 2, 3, 4, 7, 0, 5, 6, // 0b01100001 (97)
            0, 2, 3, 4, 7, 1, 5, 6, // 0b01100010 (98)
            2, 3, 4, 7, 0, 1, 5, 6, // 0b01100011 (99)
            0, 1, 3, 4, 7, 2, 5, 6, // 0b01100100 (100)
            1, 3, 4, 7, 0, 2, 5, 6, // 0b01100101 (101)
            0, 3, 4, 7, 1, 2, 5, 6, // 0b01100110 (102)
            3, 4, 7, 0, 1, 2, 5, 6, // 0b01100111 (103)
            0, 1, 2, 4, 7, 3, 5, 6, // 0b01101000 (104)
            1, 2, 4, 7, 0, 3, 5, 6, // 0b01101001 (105)
            0, 2, 4, 7, 1, 3, 5, 6, // 0b01101010 (106)
            2, 4, 7, 0, 1, 3, 5, 6, // 0b01101011 (107)
            0, 1, 4, 7, 2, 3, 5, 6, // 0b01101100 (108)
            1, 4, 7, 0, 2, 3, 5, 6, // 0b01101101 (109)
            0, 4, 7, 1, 2, 3, 5, 6, // 0b01101110 (110)
            4, 7, 0, 1, 2, 3, 5, 6, // 0b01101111 (111)
            0, 1, 2, 3, 7, 4, 5, 6, // 0b01110000 (112)
            1, 2, 3, 7, 0, 4, 5, 6, // 0b01110001 (113)
            0, 2, 3, 7, 1, 4, 5, 6, // 0b01110010 (114)
            2, 3, 7, 0, 1, 4, 5, 6, // 0b01110011 (115)
            0, 1, 3, 7, 2, 4, 5, 6, // 0b01110100 (116)
            1, 3, 7, 0, 2, 4, 5, 6, // 0b01110101 (117)
            0, 3, 7, 1, 2, 4, 5, 6, // 0b01110110 (118)
            3, 7, 0, 1, 2, 4, 5, 6, // 0b01110111 (119)
            0, 1, 2, 7, 3, 4, 5, 6, // 0b01111000 (120)
            1, 2, 7, 0, 3, 4, 5, 6, // 0b01111001 (121)
            0, 2, 7, 1, 3, 4, 5, 6, // 0b01111010 (122)
            2, 7, 0, 1, 3, 4, 5, 6, // 0b01111011 (123)
            0, 1, 7, 2, 3, 4, 5, 6, // 0b01111100 (124)
            1, 7, 0, 2, 3, 4, 5, 6, // 0b01111101 (125)
            0, 7, 1, 2, 3, 4, 5, 6, // 0b01111110 (126)
            7, 0, 1, 2, 3, 4, 5, 6, // 0b01111111 (127)
            0, 1, 2, 3, 4, 5, 6, 7, // 0b10000000 (128)
            1, 2, 3, 4, 5, 6, 0, 7, // 0b10000001 (129)
            0, 2, 3, 4, 5, 6, 1, 7, // 0b10000010 (130)
            2, 3, 4, 5, 6, 0, 1, 7, // 0b10000011 (131)
            0, 1, 3, 4, 5, 6, 2, 7, // 0b10000100 (132)
            1, 3, 4, 5, 6, 0, 2, 7, // 0b10000101 (133)
            0, 3, 4, 5, 6, 1, 2, 7, // 0b10000110 (134)
            3, 4, 5, 6, 0, 1, 2, 7, // 0b10000111 (135)
            0, 1, 2, 4, 5, 6, 3, 7, // 0b10001000 (136)
            1, 2, 4, 5, 6, 0, 3, 7, // 0b10001001 (137)
            0, 2, 4, 5, 6, 1, 3, 7, // 0b10001010 (138)
            2, 4, 5, 6, 0, 1, 3, 7, // 0b10001011 (139)
            0, 1, 4, 5, 6, 2, 3, 7, // 0b10001100 (140)
            1, 4, 5, 6, 0, 2, 3, 7, // 0b10001101 (141)
            0, 4, 5, 6, 1, 2, 3, 7, // 0b10001110 (142)
            4, 5, 6, 0, 1, 2, 3, 7, // 0b10001111 (143)
            0, 1, 2, 3, 5, 6, 4, 7, // 0b10010000 (144)
            1, 2, 3, 5, 6, 0, 4, 7, // 0b10010001 (145)
            0, 2, 3, 5, 6, 1, 4, 7, // 0b10010010 (146)
            2, 3, 5, 6, 0, 1, 4, 7, // 0b10010011 (147)
            0, 1, 3, 5, 6, 2, 4, 7, // 0b10010100 (148)
            1, 3, 5, 6, 0, 2, 4, 7, // 0b10010101 (149)
            0, 3, 5, 6, 1, 2, 4, 7, // 0b10010110 (150)
            3, 5, 6, 0, 1, 2, 4, 7, // 0b10010111 (151)
            0, 1, 2, 5, 6, 3, 4, 7, // 0b10011000 (152)
            1, 2, 5, 6, 0, 3, 4, 7, // 0b10011001 (153)
            0, 2, 5, 6, 1, 3, 4, 7, // 0b10011010 (154)
            2, 5, 6, 0, 1, 3, 4, 7, // 0b10011011 (155)
            0, 1, 5, 6, 2, 3, 4, 7, // 0b10011100 (156)
            1, 5, 6, 0, 2, 3, 4, 7, // 0b10011101 (157)
            0, 5, 6, 1, 2, 3, 4, 7, // 0b10011110 (158)
            5, 6, 0, 1, 2, 3, 4, 7, // 0b10011111 (159)
            0, 1, 2, 3, 4, 6, 5, 7, // 0b10100000 (160)
            1, 2, 3, 4, 6, 0, 5, 7, // 0b10100001 (161)
            0, 2, 3, 4, 6, 1, 5, 7, // 0b10100010 (162)
            2, 3, 4, 6, 0, 1, 5, 7, // 0b10100011 (163)
            0, 1, 3, 4, 6, 2, 5, 7, // 0b10100100 (164)
            1, 3, 4, 6, 0, 2, 5, 7, // 0b10100101 (165)
            0, 3, 4, 6, 1, 2, 5, 7, // 0b10100110 (166)
            3, 4, 6, 0, 1, 2, 5, 7, // 0b10100111 (167)
            0, 1, 2, 4, 6, 3, 5, 7, // 0b10101000 (168)
            1, 2, 4, 6, 0, 3, 5, 7, // 0b10101001 (169)
            0, 2, 4, 6, 1, 3, 5, 7, // 0b10101010 (170)
            2, 4, 6, 0, 1, 3, 5, 7, // 0b10101011 (171)
            0, 1, 4, 6, 2, 3, 5, 7, // 0b10101100 (172)
            1, 4, 6, 0, 2, 3, 5, 7, // 0b10101101 (173)
            0, 4, 6, 1, 2, 3, 5, 7, // 0b10101110 (174)
            4, 6, 0, 1, 2, 3, 5, 7, // 0b10101111 (175)
            0, 1, 2, 3, 6, 4, 5, 7, // 0b10110000 (176)
            1, 2, 3, 6, 0, 4, 5, 7, // 0b10110001 (177)
            0, 2, 3, 6, 1, 4, 5, 7, // 0b10110010 (178)
            2, 3, 6, 0, 1, 4, 5, 7, // 0b10110011 (179)
            0, 1, 3, 6, 2, 4, 5, 7, // 0b10110100 (180)
            1, 3, 6, 0, 2, 4, 5, 7, // 0b10110101 (181)
            0, 3, 6, 1, 2, 4, 5, 7, // 0b10110110 (182)
            3, 6, 0, 1, 2, 4, 5, 7, // 0b10110111 (183)
            0, 1, 2, 6, 3, 4, 5, 7, // 0b10111000 (184)
            1, 2, 6, 0, 3, 4, 5, 7, // 0b10111001 (185)
            0, 2, 6, 1, 3, 4, 5, 7, // 0b10111010 (186)
            2, 6, 0, 1, 3, 4, 5, 7, // 0b10111011 (187)
            0, 1, 6, 2, 3, 4, 5, 7, // 0b10111100 (188)
            1, 6, 0, 2, 3, 4, 5, 7, // 0b10111101 (189)
            0, 6, 1, 2, 3, 4, 5, 7, // 0b10111110 (190)
            6, 0, 1, 2, 3, 4, 5, 7, // 0b10111111 (191)
            0, 1, 2, 3, 4, 5, 6, 7, // 0b11000000 (192)
            1, 2, 3, 4, 5, 0, 6, 7, // 0b11000001 (193)
            0, 2, 3, 4, 5, 1, 6, 7, // 0b11000010 (194)
            2, 3, 4, 5, 0, 1, 6, 7, // 0b11000011 (195)
            0, 1, 3, 4, 5, 2, 6, 7, // 0b11000100 (196)
            1, 3, 4, 5, 0, 2, 6, 7, // 0b11000101 (197)
            0, 3, 4, 5, 1, 2, 6, 7, // 0b11000110 (198)
            3, 4, 5, 0, 1, 2, 6, 7, // 0b11000111 (199)
            0, 1, 2, 4, 5, 3, 6, 7, // 0b11001000 (200)
            1, 2, 4, 5, 0, 3, 6, 7, // 0b11001001 (201)
            0, 2, 4, 5, 1, 3, 6, 7, // 0b11001010 (202)
            2, 4, 5, 0, 1, 3, 6, 7, // 0b11001011 (203)
            0, 1, 4, 5, 2, 3, 6, 7, // 0b11001100 (204)
            1, 4, 5, 0, 2, 3, 6, 7, // 0b11001101 (205)
            0, 4, 5, 1, 2, 3, 6, 7, // 0b11001110 (206)
            4, 5, 0, 1, 2, 3, 6, 7, // 0b11001111 (207)
            0, 1, 2, 3, 5, 4, 6, 7, // 0b11010000 (208)
            1, 2, 3, 5, 0, 4, 6, 7, // 0b11010001 (209)
            0, 2, 3, 5, 1, 4, 6, 7, // 0b11010010 (210)
            2, 3, 5, 0, 1, 4, 6, 7, // 0b11010011 (211)
            0, 1, 3, 5, 2, 4, 6, 7, // 0b11010100 (212)
            1, 3, 5, 0, 2, 4, 6, 7, // 0b11010101 (213)
            0, 3, 5, 1, 2, 4, 6, 7, // 0b11010110 (214)
            3, 5, 0, 1, 2, 4, 6, 7, // 0b11010111 (215)
            0, 1, 2, 5, 3, 4, 6, 7, // 0b11011000 (216)
            1, 2, 5, 0, 3, 4, 6, 7, // 0b11011001 (217)
            0, 2, 5, 1, 3, 4, 6, 7, // 0b11011010 (218)
            2, 5, 0, 1, 3, 4, 6, 7, // 0b11011011 (219)
            0, 1, 5, 2, 3, 4, 6, 7, // 0b11011100 (220)
            1, 5, 0, 2, 3, 4, 6, 7, // 0b11011101 (221)
            0, 5, 1, 2, 3, 4, 6, 7, // 0b11011110 (222)
            5, 0, 1, 2, 3, 4, 6, 7, // 0b11011111 (223)
            0, 1, 2, 3, 4, 5, 6, 7, // 0b11100000 (224)
            1, 2, 3, 4, 0, 5, 6, 7, // 0b11100001 (225)
            0, 2, 3, 4, 1, 5, 6, 7, // 0b11100010 (226)
            2, 3, 4, 0, 1, 5, 6, 7, // 0b11100011 (227)
            0, 1, 3, 4, 2, 5, 6, 7, // 0b11100100 (228)
            1, 3, 4, 0, 2, 5, 6, 7, // 0b11100101 (229)
            0, 3, 4, 1, 2, 5, 6, 7, // 0b11100110 (230)
            3, 4, 0, 1, 2, 5, 6, 7, // 0b11100111 (231)
            0, 1, 2, 4, 3, 5, 6, 7, // 0b11101000 (232)
            1, 2, 4, 0, 3, 5, 6, 7, // 0b11101001 (233)
            0, 2, 4, 1, 3, 5, 6, 7, // 0b11101010 (234)
            2, 4, 0, 1, 3, 5, 6, 7, // 0b11101011 (235)
            0, 1, 4, 2, 3, 5, 6, 7, // 0b11101100 (236)
            1, 4, 0, 2, 3, 5, 6, 7, // 0b11101101 (237)
            0, 4, 1, 2, 3, 5, 6, 7, // 0b11101110 (238)
            4, 0, 1, 2, 3, 5, 6, 7, // 0b11101111 (239)
            0, 1, 2, 3, 4, 5, 6, 7, // 0b11110000 (240)
            1, 2, 3, 0, 4, 5, 6, 7, // 0b11110001 (241)
            0, 2, 3, 1, 4, 5, 6, 7, // 0b11110010 (242)
            2, 3, 0, 1, 4, 5, 6, 7, // 0b11110011 (243)
            0, 1, 3, 2, 4, 5, 6, 7, // 0b11110100 (244)
            1, 3, 0, 2, 4, 5, 6, 7, // 0b11110101 (245)
            0, 3, 1, 2, 4, 5, 6, 7, // 0b11110110 (246)
            3, 0, 1, 2, 4, 5, 6, 7, // 0b11110111 (247)
            0, 1, 2, 3, 4, 5, 6, 7, // 0b11111000 (248)
            1, 2, 0, 3, 4, 5, 6, 7, // 0b11111001 (249)
            0, 2, 1, 3, 4, 5, 6, 7, // 0b11111010 (250)
            2, 0, 1, 3, 4, 5, 6, 7, // 0b11111011 (251)
            0, 1, 2, 3, 4, 5, 6, 7, // 0b11111100 (252)
            1, 0, 2, 3, 4, 5, 6, 7, // 0b11111101 (253)
            0, 1, 2, 3, 4, 5, 6, 7, // 0b11111110 (254)
            0, 1, 2, 3, 4, 5, 6, 7, // 0b11111111 (255)
            0, 0, 0, 0, 0, 0, 0, 0, // Ensuring we cannot overrun the buffer.
        }};                
    }}"""
        print(s, file=f)

    def generate_efficient_partitioning_call(self, param):

        if self.max_inner_unroll == self.safe_inner_unroll:
            return F"""
            var sep = vectorized_partition_{self.max_inner_unroll}(left, right, hint);                                
"""
        else:
            return F"""                
            var sep = length < {self.get_configuration_constant("PartitionTempSizeInElements")} ?
                vectorized_partition_{self.max_inner_unroll}(left, right, hint) :
                vectorized_partition_{self.safe_inner_unroll}(left, right, hint);
"""

    def get_configuration_constant(self, param):
        return F"{self.capital_type_map[self.type]}Config.{param}"






